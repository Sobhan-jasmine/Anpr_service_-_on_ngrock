{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2956,"status":"ok","timestamp":1709013887572,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"},"user_tz":-210},"id":"7AdsOFdTCfR7","outputId":"8cbc5bd7-9025-4584-f776-75d63792a49c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ib7a6OuRCM-f"},"outputs":[],"source":["import os\n","# os.chdir('/content/drive/MyDrive/cheak_detaction/yolov8')\n","os.chdir('/content/drive/MyDrive/customer_service')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12207,"status":"ok","timestamp":1709013906738,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"},"user_tz":-210},"id":"f2_WH48CDL7g","outputId":"49c8dbf7-c48b-4e8c-a063-0ba7592c7f0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.1.19)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.1.1.post2209072238)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4maqynbCmDP"},"outputs":[],"source":["import glob\n","import numpy as np\n","from PIL import Image\n","import os\n","# from sklearn.model_selection import train_test_split\n","import cv2\n","import os\n","from ultralytics import YOLO\n","import shutil\n","# from skimage.io import imread_collection\n","import torch\n","# import tensorflow as tf\n","import torch.nn as nn\n","from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam\n","from torch.autograd import Variable\n","import torchvision\n","import pathlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLwjGrl8Cojn"},"outputs":[],"source":["# tf.debugging.set_log_device_placement(True)\n","\n","\n","# device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","\n","\n","model_11 = YOLO(\"Copy of first_crop.pt\")\n","# model_1.to('cuda')\n","# model_11.to(device)\n","model_2 = YOLO(\"Copy of Copy of best.pt\")\n","# model_2.to(device)\n","\n","# characterRecognition = tf.keras.models.load_model(\"Copy of model_char_recognition_4.h5\")\n","my_model = YOLO('best_seg.pt')\n","# my_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Et2W9UzdCzK-"},"outputs":[],"source":["class TinyVGG(nn.Module):\n","    \"\"\"\n","    Model architecture copying TinyVGG from:\n","    https://poloclub.github.io/cnn-explainer/\n","    \"\"\"\n","    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n","        super().__init__()\n","        self.conv_block_1 = nn.Sequential(\n","            nn.Conv2d(in_channels=input_shape,\n","                      out_channels=hidden_units,\n","                      kernel_size=3, # how big is the square that's going over the image?\n","                      stride=1, # default\n","                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=hidden_units,\n","                      out_channels=hidden_units,\n","                      kernel_size=3,\n","                      stride=1,\n","                      padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,\n","                         stride=2) # default stride value is same as kernel_size\n","        )\n","        self.conv_block_2 = nn.Sequential(\n","            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            # Where did this in_features shape come from?\n","            # It's because each layer of our network compresses and changes the shape of our inputs data.\n","            nn.Linear(in_features=hidden_units*16*16,\n","                      out_features=output_shape)\n","        )\n","\n","    def forward(self, x: torch.Tensor):\n","        x = self.conv_block_1(x)\n","        # print(x.shape)\n","        x = self.conv_block_2(x)\n","        # print(x.shape)\n","        x = self.classifier(x)\n","        # print(x.shape)\n","        return x\n","        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n","\n","# Create model_1 and send it to the target device\n","torch.manual_seed(42)\n","model_1 = TinyVGG(\n","    input_shape=3,\n","    hidden_units=10,\n","    output_shape=10)\n","\n","model_1.load_state_dict(torch.load(r'01_pytorch_workflow_model_1.pth',map_location=torch.device('cpu') ))\n","\n","\n","# def cnnCharRecognition(img):\n","#     dictionary = {0:'0', 1:'1', 2 :'2', 3:'3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8', 9:'9', 10:'A',\n","#     11:'B', 12:'C', 13:'D', 14:'E', 15:'F', 16:'G', 17:'H', 18:'I', 19:'J', 20:'K',\n","#     21:'L', 22:'M', 23:'N', 24:'P', 25:'Q', 26:'R', 27:'S', 28:'T', 29:'U',\n","#     30:'V', 31:'W', 32:'X', 33:'Y', 34:'Z'}\n","\n","#     blackAndWhiteChar=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","#     blackAndWhiteChar = cv2.resize(blackAndWhiteChar,(75,100))\n","#     image = blackAndWhiteChar.reshape((1,100,75,1))\n","#     image = image / 255.0\n","#     new_predictions = characterRecognition.predict(image)\n","#     char = np.argmax(new_predictions)\n","#     return char\n","\n","def number_recog(custom_image_path):\n","  # Load in custom image and convert the tensor values to float32\n","  custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n","\n","  # Divide the image pixel values by 255 to get them between [0, 1]\n","  custom_image = custom_image / 255.\n","  # Create transform pipleine to resize image\n","  custom_image_transform = transforms.Compose([\n","      transforms.Resize((64, 64)),\n","  ])\n","\n","  # Transform target image\n","  custom_image_transformed = custom_image_transform(custom_image)\n","  model_1.eval()\n","  with torch.inference_mode():\n","      # Add an extra dimension to image\n","      custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n","\n","      # Print out different shapes\n","      print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n","      print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n","\n","      # Make a prediction on image with an extra dimension\n","      custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0))\n","  # Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n","  custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n","  # Convert prediction probabilities -> prediction labels\n","  custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n","  class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n","  # Find the predicted label\n","  custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n","  return custom_image_pred_class\n","\n","\n","def first_crop(img):\n","      # try:\n","    os.chdir('/content/drive/MyDrive/customer_service')\n","    # detect location of plate\n","    img1 = 'images/'+img\n","    result = model_11.predict(source=img1,save_crop = True)\n","    # change directory to detected and cropd plate\n","    os.chdir('runs/detect/predict/crops/0')\n","     # detect characters and numbers\n","    img = img[:-3]+'jpg'\n","    result = model_2.predict(source=img , save_crop = True)\n","    result_char = my_model.predict(source=img )\n","    # change directory to detected and croped characters and numbers\n","    os.chdir('runs/detect/predict/crops/number')\n","\n","    #access location of detected characters\n","    for r in result:\n","      crd = r.boxes.xyxy\n","    coordinates = crd.tolist()\n","    print(coordinates)\n","\n","    lst = [img]\n","    dic = {img:coordinates[0][0]}\n","    for i in range(2,9):\n","      cropped_name = img[:-4]+ str(i) +'.jpg'\n","      dic[cropped_name] = coordinates[i-1][0]\n","\n","    print(dic)\n","    #sort croppd characters and numbers by their locations\n","    dic = {k: v for k, v in sorted(dic.items(), key=lambda item: item[1])}\n","\n","    #character recognetion and classification\n","    lst_rs=[]\n","    for i in dic:\n","      custom_image_path = i\n","      lst_rs.append(number_recog(custom_image_path))\n","      # print(cnnCharRecognition(im))\n","      print(i)\n","      print(\"-----------------\")\n","    names = {0: 'a', 1: 'b', 2: 'd', 3: 'eight', 4: 'ein', 5: 'five', 6: 'four',\n","          7: 'ghaf', 8: 'h', 9: 'jim', 10: 'lam', 11: 'mim', 12: 'nine',\n","          13: 'non', 14: 'one', 15: 'sad', 16: 'seven', 17: 'sin', 18: 'six', 19: 'ta',\n","          20: 'three', 21: 'two', 22: 'waw', 23: 'wheel', 24: 'y', 25: 'zero'}\n","    for r in result_char:\n","      cls_names = r.boxes.cls.tolist()\n","\n","    lst = [0,1,2,4,7,8,9,10,11,12,13,14,15,17,19,22,23,24]\n","    for i in lst :\n","      if i in cls_names :\n","        lst_rs[2] = names[i]\n","    # lst_rs[2] =  charac\n","    # Deleting an non-empty folder\n","    dir_path = r\"/content/drive/MyDrive/customer_service/runs\"\n","    shutil.rmtree(dir_path, ignore_errors=True)\n","    os.chdir('/content/drive/MyDrive/customer_service')\n","    return(str(lst_rs))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":887},"executionInfo":{"elapsed":12301,"status":"ok","timestamp":1709012978942,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"},"user_tz":-210},"id":"W_GPbtR6DrP_","outputId":"a21c1f3e-1528-4bd3-bffc-130d571ca345"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/drive/MyDrive/customer_service/images/65ya61677.png: 480x640 1 0, 1497.5ms\n","Speed: 9.8ms preprocess, 1497.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict/crops/0/65ya61677.jpg: 288x640 8 numbers, 632.1ms\n","Speed: 1.9ms preprocess, 632.1ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict/crops/0/65ya61677.jpg: 288x640 1 y, 2455.9ms\n","Speed: 1.5ms preprocess, 2455.9ms inference, 3.3ms postprocess per image at shape (1, 3, 288, 640)\n","[[98.92752075195312, 21.81349754333496, 109.53571319580078, 46.82591247558594], [110.79866027832031, 22.025680541992188, 121.64578247070312, 46.629478454589844], [20.992311477661133, 14.450210571289062, 29.895048141479492, 41.38693618774414], [61.67079162597656, 16.0031795501709, 72.43990325927734, 44.542335510253906], [84.65768432617188, 16.27770233154297, 95.71847534179688, 45.419490814208984], [73.83395385742188, 16.51178550720215, 83.53316497802734, 45.008575439453125], [31.320112228393555, 14.619946479797363, 42.52162551879883, 41.965415954589844], [44.0848388671875, 16.25649070739746, 60.1860237121582, 43.156131744384766]]\n","{'65ya61677.jpg': 98.92752075195312, '65ya616772.jpg': 110.79866027832031, '65ya616773.jpg': 20.992311477661133, '65ya616774.jpg': 61.67079162597656, '65ya616775.jpg': 84.65768432617188, '65ya616776.jpg': 73.83395385742188, '65ya616777.jpg': 31.320112228393555, '65ya616778.jpg': 44.0848388671875}\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616773.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616777.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616778.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616774.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616776.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616775.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya61677.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616772.jpg\n","-----------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["\"['6', '5', 'y', '6', '1', '6', '7', '7']\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["#dont run\n","first_crop('65ya61677.png')"]},{"cell_type":"code","source":["import os\n","# os.chdir('/content/drive/MyDrive/cheak_detaction/yolov8')\n","os.chdir('/content/drive/MyDrive/customer_service')"],"metadata":{"id":"XsPaWpIYHSQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8176,"status":"ok","timestamp":1709013956043,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"},"user_tz":-210},"id":"5Z1QVMC4HzBE","outputId":"cf949527-f10e-49c7-8fd5-b210f525db1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.110.0)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.27.1)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.6.1)\n","Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.36.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.9.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.16.2)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi) (3.7.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (3.6)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.2.0)\n"]}],"source":["!pip install fastapi uvicorn pyngrok\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14958,"status":"ok","timestamp":1709013976892,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"},"user_tz":-210},"id":"tSPtjgIGIwgt","outputId":"520d4bdb-4f5a-44a3-8ac9-aa329f577805"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.9)\n"]}],"source":["!pip install python-multipart"]},{"cell_type":"code","source":["from fastapi import FastAPI, Request, UploadFile, File\n","from fastapi.responses import HTMLResponse\n","from fastapi.templating import Jinja2Templates\n","from fastapi.staticfiles import StaticFiles\n","# from Anpr import read_imgs\n","# from Anpr import first_crop\n","# from test import first_crop\n","import cv2\n","# from cv2  import dnn_superres\n","import torch\n","# from skimage.io import imread_collection\n","from ultralytics import YOLO\n","# from cv2 import dnn_superres\n","import uvicorn\n","from typing import List\n","from pathlib import Path\n","from fastapi.responses import FileResponse\n","from ultralytics import YOLO\n","\n","\n","import os\n","\n","\n","# device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","\n","# model_1 = YOLO(\"Copy of first_crop.pt\")\n","# model_1.to(device)\n","# # model_2 = YOLO(\"Copy of Copy of best.pt\")\n","# # model_2.to(device)\n","\n","\n","# characterRecognition = tf.keras.models.load_model(\"Copy of model_char_recognition_4.h5\")\n","# my_model = YOLO('best_seg.pt')\n","# my_model.to(device)\n","# yolo = torch.hub.load('ultralytics/yolov5', 'custom', path='best.pt')\n","\n","# my_model = YOLO('weights_segment/best.pt')\n","IMAGEDIR = \"images/\"\n","\n","app = FastAPI()\n","# BASE_DIR = Path(__file__).resolve().parent\n","\n","# templates = Jinja2Templates(directory=str(Path(BASE_DIR, 'templates')))\n","templates = Jinja2Templates(directory=\"templates\")\n","app.mount(\"/images\", StaticFiles(directory=\"images\"), name=\"images\")\n","\n","@app.get('/', response_class=HTMLResponse)\n","def home(request: Request):\n","    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n","\n","\n","@app.post(\"/images\")\n","def images(img1: bytes = File(...), img2: bytes = File(...)):\n","    # **do something**\n","    return {\"message\": \"OK\"}\n","\n","\n","\n","@app.post(\"/upload-files\")\n","async def create_upload_files(request: Request, files: List[UploadFile] = File(...)):\n","    for file in files:\n","        contents = await file.read()\n","        #save the file\n","        os.chdir('/content/drive/MyDrive/customer_service')\n","        name = file.filename\n","        with open(f\"{IMAGEDIR}{file.filename}\", \"wb\") as f:\n","            f.write(contents)\n","\n","    # show = [file.filename for file in files]\n","    # resultt  = read_imgs(name)\n","\n","\n","    return show_result(Request,name)\n","    #return {\"Result\": \"OK\", \"filenames\": [file.filename for file in files]}\n","    # return resultt\n","\n","\n","\n","@app.get(\"/show-files\",response_class=HTMLResponse)\n","def show_result(request: Request,name):\n","    resultt=first_crop(name)\n","    result_dic={'numbers':resultt,'img_name':name}\n","    # return templates.TemplateResponse(\"show.html\" ,{\"request\":Request,\"res\":result_dic})\n","    # return templates.TemplateResponse(\"show.html\" ,{\"request\":Request,\"resultt\":resultt,\"name\":name})\n","    return resultt\n","\n","# if __name__ == '__main__':\n","#     uvicorn.run(app, port=8000,host='127.0.0.1')"],"metadata":{"id":"ssIfNOvd7QnL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1251,"status":"ok","timestamp":1709013989603,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"},"user_tz":-210},"id":"H7l1xRqBI-ko","outputId":"05722f9c-7f41-43a8-a4aa-08ea85c638d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}],"source":["!ngrok config add-authtoken 2ZniBIHz91cZTaPXjlSX1o9ydam_5Sfgm5Edg7ZtaN829Mn7E"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOEjI4z7I5Zt","outputId":"b8cbfd65-b1d9-4f0a-8a7e-899000cc52bc","executionInfo":{"status":"ok","timestamp":1709031563894,"user_tz":-210,"elapsed":17567408,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Public URL: https://a8cb-34-106-0-38.ngrok-free.app\n"]},{"output_type":"stream","name":"stderr","text":["INFO:     Started server process [19401]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:     37.32.121.233:0 - \"GET / HTTP/1.1\" 200 OK\n","INFO:     37.32.121.233:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n","INFO:     37.32.121.233:0 - \"POST /upload-files/ HTTP/1.1\" 307 Temporary Redirect\n","INFO:     37.32.121.233:0 - \"POST /upload-files/ HTTP/1.1\" 307 Temporary Redirect\n","\n","image 1/1 /content/drive/MyDrive/customer_service/images/411643_2.jpg: 480x640 1 0, 1104.3ms\n","Speed: 2.5ms preprocess, 1104.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict/crops/0/411643_2.jpg: 288x640 8 numbers, 1063.8ms\n","Speed: 1.8ms preprocess, 1063.8ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict/crops/0/411643_2.jpg: 288x640 1 y, 2409.0ms\n","Speed: 1.7ms preprocess, 2409.0ms inference, 4.7ms postprocess per image at shape (1, 3, 288, 640)\n","[[121.28026580810547, 27.134708404541016, 132.98471069335938, 51.84364700317383], [45.98710632324219, 17.940845489501953, 65.60784912109375, 45.27702713012695], [67.18043518066406, 18.495927810668945, 78.48747253417969, 47.50423812866211], [79.271240234375, 19.250730514526367, 90.94451904296875, 48.304141998291016], [33.542701721191406, 16.0616455078125, 44.17936325073242, 44.29077911376953], [23.090837478637695, 15.694987297058105, 32.824798583984375, 43.66643524169922], [107.98606872558594, 26.26321029663086, 120.09124755859375, 51.6342658996582], [91.72119140625, 20.332965850830078, 103.25375366210938, 48.9980583190918]]\n","{'411643_2.jpg': 121.28026580810547, '411643_22.jpg': 45.98710632324219, '411643_23.jpg': 67.18043518066406, '411643_24.jpg': 79.271240234375, '411643_25.jpg': 33.542701721191406, '411643_26.jpg': 23.090837478637695, '411643_27.jpg': 107.98606872558594, '411643_28.jpg': 91.72119140625}\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","411643_26.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","411643_25.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","411643_22.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","411643_23.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","411643_24.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","411643_28.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","411643_27.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","411643_2.jpg\n","-----------------\n","INFO:     37.32.121.233:0 - \"POST /upload-files HTTP/1.1\" 200 OK\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["INFO:     37.32.121.233:0 - \"GET / HTTP/1.1\" 200 OK\n","INFO:     37.32.121.233:0 - \"POST /upload-files/ HTTP/1.1\" 307 Temporary Redirect\n","\n","image 1/1 /content/drive/MyDrive/customer_service/images/214351_2.jpg: 480x640 1 0, 1029.8ms\n","Speed: 2.0ms preprocess, 1029.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict/crops/0/214351_2.jpg: 288x640 8 numbers, 641.0ms\n","Speed: 1.9ms preprocess, 641.0ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict/crops/0/214351_2.jpg: 288x640 1 y, 2352.2ms\n","Speed: 2.0ms preprocess, 2352.2ms inference, 2.9ms postprocess per image at shape (1, 3, 288, 640)\n","[[98.92752075195312, 21.81349754333496, 109.53571319580078, 46.82591247558594], [110.79866027832031, 22.025680541992188, 121.64578247070312, 46.629478454589844], [20.992311477661133, 14.450210571289062, 29.895048141479492, 41.38693618774414], [61.67079162597656, 16.0031795501709, 72.43990325927734, 44.542335510253906], [84.65768432617188, 16.27770233154297, 95.71847534179688, 45.419490814208984], [73.83395385742188, 16.51178550720215, 83.53316497802734, 45.008575439453125], [31.320112228393555, 14.619946479797363, 42.52162551879883, 41.965415954589844], [44.0848388671875, 16.25649070739746, 60.1860237121582, 43.156131744384766]]\n","{'214351_2.jpg': 98.92752075195312, '214351_22.jpg': 110.79866027832031, '214351_23.jpg': 20.992311477661133, '214351_24.jpg': 61.67079162597656, '214351_25.jpg': 84.65768432617188, '214351_26.jpg': 73.83395385742188, '214351_27.jpg': 31.320112228393555, '214351_28.jpg': 44.0848388671875}\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_23.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_27.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_28.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_24.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_26.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_25.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_2.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_22.jpg\n","-----------------\n","INFO:     37.32.121.233:0 - \"POST /upload-files HTTP/1.1\" 200 OK\n","INFO:     37.32.121.233:0 - \"POST /upload-files/ HTTP/1.1\" 307 Temporary Redirect\n","\n","image 1/1 /content/drive/MyDrive/customer_service/images/214351_2.jpg: 480x640 1 0, 1885.8ms\n","Speed: 27.5ms preprocess, 1885.8ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict/crops/0/214351_2.jpg: 288x640 8 numbers, 1104.9ms\n","Speed: 2.4ms preprocess, 1104.9ms inference, 1.3ms postprocess per image at shape (1, 3, 288, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict/crops/0/214351_2.jpg: 288x640 1 y, 2883.8ms\n","Speed: 2.0ms preprocess, 2883.8ms inference, 4.1ms postprocess per image at shape (1, 3, 288, 640)\n","[[98.92752075195312, 21.81349754333496, 109.53571319580078, 46.82591247558594], [110.79866027832031, 22.025680541992188, 121.64578247070312, 46.629478454589844], [20.992311477661133, 14.450210571289062, 29.895048141479492, 41.38693618774414], [61.67079162597656, 16.0031795501709, 72.43990325927734, 44.542335510253906], [84.65768432617188, 16.27770233154297, 95.71847534179688, 45.419490814208984], [73.83395385742188, 16.51178550720215, 83.53316497802734, 45.008575439453125], [31.320112228393555, 14.619946479797363, 42.52162551879883, 41.965415954589844], [44.0848388671875, 16.25649070739746, 60.1860237121582, 43.156131744384766]]\n","{'214351_2.jpg': 98.92752075195312, '214351_22.jpg': 110.79866027832031, '214351_23.jpg': 20.992311477661133, '214351_24.jpg': 61.67079162597656, '214351_25.jpg': 84.65768432617188, '214351_26.jpg': 73.83395385742188, '214351_27.jpg': 31.320112228393555, '214351_28.jpg': 44.0848388671875}\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_23.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_27.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_28.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_24.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_26.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_25.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_2.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","214351_22.jpg\n","-----------------\n","INFO:     37.32.121.233:0 - \"POST /upload-files HTTP/1.1\" 200 OK\n"]},{"output_type":"stream","name":"stderr","text":["INFO:     Shutting down\n","INFO:     Waiting for application shutdown.\n","INFO:     Application shutdown complete.\n","INFO:     Finished server process [19401]\n"]}],"source":["import nest_asyncio\n","from pyngrok import ngrok\n","import uvicorn\n","\n","ngrok_tunnel = ngrok.connect(8000)\n","print('Public URL:', ngrok_tunnel.public_url)\n","nest_asyncio.apply()\n","uvicorn.run(app, port=8000)"]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHTnqDb1H0NH","executionInfo":{"status":"error","timestamp":1709013204524,"user_tz":-210,"elapsed":368,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"}},"outputId":"d0f4112d-633a-4a9b-c4f5-b78240e30c7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["UsageError: CWD no longer exists - please use %cd to change directory.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1708979076246,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"},"user_tz":-210},"id":"E6Vf6co8Fqr-","outputId":"6d587612-bbf9-4e3a-f509-b9276c6af9a5"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'65ya61677.jpg'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["img = '65ya61677.png'\n","img = img[:-3]+'jpg'\n","img"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":852},"executionInfo":{"elapsed":6114,"status":"ok","timestamp":1708979307450,"user":{"displayName":"sobttm ttm","userId":"12784982686433885696"},"user_tz":-210},"id":"f4qVx-raC2Na","outputId":"08a25b8a-28f3-4b64-cbb7-cc5c9e547a31"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","image 1/1 /content/drive/MyDrive/customer_service/images/65ya61677.png: 480x640 1 0, 1013.5ms\n","Speed: 2.8ms preprocess, 1013.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n","Results saved to \u001b[1mruns/detect/predict3\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict3/crops/0/65ya61677.jpg: 288x640 8 numbers, 616.1ms\n","Speed: 1.8ms preprocess, 616.1ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","\n","image 1/1 /content/drive/MyDrive/customer_service/runs/detect/predict3/crops/0/65ya61677.jpg: 288x640 1 y, 3531.2ms\n","Speed: 1.8ms preprocess, 3531.2ms inference, 4.6ms postprocess per image at shape (1, 3, 288, 640)\n","[[98.92752075195312, 21.81349754333496, 109.53571319580078, 46.82591247558594], [110.79866027832031, 22.025680541992188, 121.64578247070312, 46.629478454589844], [20.992311477661133, 14.450210571289062, 29.895048141479492, 41.38693618774414], [61.67079162597656, 16.0031795501709, 72.43990325927734, 44.542335510253906], [84.65768432617188, 16.27770233154297, 95.71847534179688, 45.419490814208984], [73.83395385742188, 16.51178550720215, 83.53316497802734, 45.008575439453125], [31.320112228393555, 14.619946479797363, 42.52162551879883, 41.965415954589844], [44.0848388671875, 16.25649070739746, 60.1860237121582, 43.156131744384766]]\n","{'65ya61677.jpg': 98.92752075195312, '65ya616772.jpg': 110.79866027832031, '65ya616773.jpg': 20.992311477661133, '65ya616774.jpg': 61.67079162597656, '65ya616775.jpg': 84.65768432617188, '65ya616776.jpg': 73.83395385742188, '65ya616777.jpg': 31.320112228393555, '65ya616778.jpg': 44.0848388671875}\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616773.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616777.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616778.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616774.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616776.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616775.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya61677.jpg\n","-----------------\n","Custom image transformed shape: torch.Size([3, 64, 64])\n","Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n","65ya616772.jpg\n","-----------------\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"['6', '5', 'y', '6', '1', '6', '7', '7']\""]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# first_crop('65ya61677.png')\n","img  = '65ya61677.png'\n","# try:\n","os.chdir('/content/drive/MyDrive/customer_service')\n","# detect location of plate\n","img1 = 'images/'+img\n","result = model_11.predict(source=img1,save_crop = True)\n","# change directory to detected and cropd plate\n","os.chdir('runs/detect/predict3/crops/0')\n","  # detect characters and numbers\n","img = img[:-3]+'jpg'\n","result = model_2.predict(source=img , save_crop = True)\n","result_char = my_model.predict(source=img )\n","# change directory to detected and croped characters and numbers\n","os.chdir('runs/detect/predict/crops/number')\n","\n","#access location of detected characters\n","for r in result:\n","  crd = r.boxes.xyxy\n","coordinates = crd.tolist()\n","print(coordinates)\n","\n","lst = [img]\n","dic = {img:coordinates[0][0]}\n","for i in range(2,9):\n","  cropped_name = img[:-4]+ str(i) +'.jpg'\n","  dic[cropped_name] = coordinates[i-1][0]\n","\n","print(dic)\n","#sort croppd characters and numbers by their locations\n","dic = {k: v for k, v in sorted(dic.items(), key=lambda item: item[1])}\n","\n","#character recognetion and classification\n","lst_rs=[]\n","for i in dic:\n","  custom_image_path = i\n","  lst_rs.append(number_recog(custom_image_path))\n","  # print(cnnCharRecognition(im))\n","  print(i)\n","  print(\"-----------------\")\n","names = {0: 'a', 1: 'b', 2: 'd', 3: 'eight', 4: 'ein', 5: 'five', 6: 'four',\n","      7: 'ghaf', 8: 'h', 9: 'jim', 10: 'lam', 11: 'mim', 12: 'nine',\n","      13: 'non', 14: 'one', 15: 'sad', 16: 'seven', 17: 'sin', 18: 'six', 19: 'ta',\n","      20: 'three', 21: 'two', 22: 'waw', 23: 'wheel', 24: 'y', 25: 'zero'}\n","for r in result_char:\n","  cls_names = r.boxes.cls.tolist()\n","\n","lst = [0,1,2,4,7,8,9,10,11,12,13,14,15,17,19,22,23,24]\n","for i in lst :\n","  if i in cls_names :\n","    lst_rs[2] = names[i]\n","# lst_rs[2] =  charac\n","# Deleting an non-empty folder\n","dir_path = r\"/content/drive/MyDrive/customer_service/runs\"\n","shutil.rmtree(dir_path, ignore_errors=True)\n","str(lst_rs)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOjuCeIufQvvQP/+RYiVYAL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}